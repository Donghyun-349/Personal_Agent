import os
import sys
from pathlib import Path
import logging
import re
from datetime import datetime
from urllib.parse import urlparse
from dotenv import load_dotenv

# Add project root to path
sys.path.append(os.getcwd())

from src.utils import ImageProcessor, ConfigManager
from src.generators import HTMLGenerator, PDFGenerator, MarkdownGenerator
from src.clippers import WebClipper
from src.summarizer import GeminiSummarizer
from src.uploader import GDriveUploader

# Setup logging
logging.basicConfig(level=logging.INFO)
sys.stdout.reconfigure(encoding='utf-8')

def test_scraper():
    print("üöÄ Starting Local Scraper Test...")
    
    # Load environment variables
    load_dotenv()
    
    # Test URL
    target_url_env = os.getenv("TARGET_URL")
    if target_url_env:
        url = target_url_env
        print(f"üîó Using URL from Environment Variable: {url}")
    else:
        # Default Test URLs
        # url = "https://www.python.org/blogs/"
        # url = "https://blog.naver.com/tri99er/224140816612"
        # url = "https://blog.naver.com/tri99er/224140816612?test_param=123&utm_source=test"
        url = "https://blog.naver.com/tri99er/224140816612?test_param=123&utm_source=test"
        # url = "https://www.youtube.com/watch?v=gDdPs7oGRXU"
    
    print(f"Target URL: {url}")

    # Setup directories
    cwd = Path.cwd()
    test_dir = cwd / "test_output"
    test_dir.mkdir(exist_ok=True)
    assets_dir = test_dir / "assets"
    
    # Initialize Components
    image_processor = ImageProcessor(assets_dir)
    html_gen = HTMLGenerator(test_dir, assets_dir)
    pdf_gen = PDFGenerator(test_dir, assets_dir)
    md_gen = MarkdownGenerator(test_dir)
    
    # Initialize Step 2 Components (Summarizer & Uploader)
    gemini_key = os.getenv("GOOGLE_API_KEY")
    drive_token = os.getenv("GOOGLE_TOKEN_JSON")
    drive_folder_id = os.getenv("GOOGLE_DRIVE_FOLDER_ID")

    # Fallback to local files if env vars are missing
    if not drive_token:
        possible_tokens = ["credentials/token.json", "token.json"]
        for t in possible_tokens:
            if (cwd / t).exists():
                drive_token = str(cwd / t)
                print(f"‚ÑπÔ∏è  Found token file at: {drive_token}")
                break

    summarizer = None
    uploader = None

    if gemini_key:
        print("‚úÖ Gemini API Key found. Initializing Summarizer...")
        summarizer = GeminiSummarizer(gemini_key)
    else:
        print("‚ö†Ô∏è  No Gemini API Key found (GOOGLE_API_KEY). Summarization will be skipped.")

    if drive_token:
        print("‚úÖ Google Drive Token found. Initializing Uploader...")
        uploader = GDriveUploader(drive_token)
    else:
        print("‚ö†Ô∏è  No Drive Token found (GOOGLE_TOKEN_JSON). Upload will be skipped.")

    
    # Select Clipper
    if 'youtube.com' in url or 'youtu.be' in url:
        from src.clippers import YouTubeClipper
        clipper = YouTubeClipper(image_processor, log_callback=print)
        is_youtube = True
    else:
        from src.clippers import WebClipper
        clipper = WebClipper(image_processor, html_gen)
        is_youtube = False
    
    try:
        # 1. Extract
        print("\n1Ô∏è‚É£  Extracting content...")
        data = clipper.extract_content(url)
        print(f"‚úÖ Extracted Title: {data['title']}")
        print(f"‚úÖ Content Length: {len(data['content'])} chars")
        
        saved_files = []

        if is_youtube:
            # YouTube: Summary + Transcript Merge
            
            # Generate Summary First if available
            if summarizer:
                print("\n2Ô∏è‚É£  Generating Summary (Gemini - YouTube Mode)...")
                # Prepare metadata for summarizer
                metadata = {}
                if is_youtube and 'upload_date' in data:
                    # yt-dlp upload_date is usually YYYYMMDD
                    ud = data['upload_date']
                    if len(ud) == 8:
                        formatted_date = f"{ud[:4]}-{ud[4:6]}-{ud[6:]}"
                        metadata['publish_date'] = formatted_date
                
                # YouTube URL Î∂ÑÏÑù Î™®Îìú Ï†ÑÎã¨
                if data.get('use_gemini_url'):
                    metadata['use_gemini_url'] = True
                    metadata['youtube_url'] = data['url']
                
                summary = summarizer.summarize_text(data['content'], content_type='youtube', metadata=metadata)
                if summary:
                    # [Íµ¨Ï°∞ Î≥ÄÍ≤Ω] 1. ÏöîÏïΩ (Frontmatter Ìè¨Ìï®) -> 2. ÎåÄÎ≥∏ (Ïù¥ÎØ∏ÏßÄ/Ìó§Îçî Ï†úÍ±∞)
                    
                    # ÎåÄÎ≥∏ ÏÑπÏÖò Íµ¨ÏÑ± (Ìó§Îçî Ï†úÍ±∞, Íµ¨Î∂ÑÏÑ†Îßå Ï∂îÍ∞Ä)
                    # "ÏûêÎßâ Î∂ÄÎ∂ÑÎßå Ïú†ÏßÄ" -> data['content']
                    transcript_section = f"\n\n---\n\n{data['content']}"
                    
                    # Ï†ÑÏ≤¥ ÎÇ¥Ïö© Î≥ëÌï© (ÏöîÏïΩÎ≥∏Ïù¥ Ïù¥ÎØ∏ FrontmatterÏôÄ Ï†úÎ™©ÏùÑ Ìè¨Ìï®ÌïòÍ≥† ÏûàÏùå)
                    data['content'] = f"{summary}{transcript_section}"
                    print("‚úÖ Summary merged into transcript.")
                else:
                    print("‚ùå Summary generation failed.")
            
            # Save Combined MD
            print("\n3Ô∏è‚É£  Saving Transcript (with Summary)...")
            
            # ÌååÏùºÎ™Ö ÏÉùÏÑ±: ÎÇ†Ïßú Ï†úÍ±∞ (YouTubeÎßå)
            # MarkdownGenerator.generate_filenameÏùÄ ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú utils.generate_filenameÏùÑ Ìò∏Ï∂úÌïòÎäîÎç∞
            # ÎÇ†ÏßúÎ•º Í∞ïÏ†úÎ°ú ÎÑ£ÏúºÎØÄÎ°ú, Ïó¨Í∏∞ÏÑú filenameÏùÑ Ïò§Î≤ÑÎùºÏù¥ÎìúÌï† Î∞©Î≤ïÏù¥ ÌïÑÏöîÌï®.
            # ÌïòÏßÄÎßå MarkdownGenerator.saveÎäî ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú generate_filenameÏùÑ Ìò∏Ï∂ú.
            # ÏûÑÏãú Ìï¥Í≤∞Ï±Ö: data['title']Ïóê ÎÇ†ÏßúÍ∞Ä Ïïà Îì§Ïñ¥Í∞ÄÎèÑÎ°ù ÌïòÍ≥†, generator Ï∏°Î©¥ÏóêÏÑú ÎÇ†Ïßú prefix Î°úÏßÅÏùÑ Ïö∞ÌöåÌïòÍ±∞ÎÇò
            # GeneratorÍ∞Ä Ï†úÍ≥µÌïòÎäî Ïú†Ïó∞ÏÑ±Ïù¥ Î∂ÄÏ°±ÌïòÎ©¥ ÏßÅÏ†ë Ï†ÄÏû• Î°úÏßÅÏùÑ Íµ¨ÌòÑÌï¥Ïïº Ìï† ÏàòÎèÑ ÏûàÏùå.
            # ÌòÑÏû¨ utils.generate_filenameÏù¥ ÎÇ†ÏßúÎ•º Î∞ïÏïÑÎ≤ÑÎ¶º. -> [2026-01-11] ...
            
            # generators.pyÎ•º ÏàòÏ†ïÌïòÏßÄ ÏïäÍ≥† ÌååÏùºÎ™ÖÏùÑ Ï†úÏñ¥ÌïòÍ∏∞ Ïñ¥Î†§ÏõÄ.
            # save Î©îÏÑúÎìúÎ•º Ìò∏Ï∂úÌïòÎêò, Í≤∞Í≥º ÌååÏùºÎ™ÖÏùÑ Î≥ÄÍ≤ΩÌïòÎäî Î∞©Ïãù ÏÇ¨Ïö©
            md_path = md_gen.save(data, image_processor=image_processor)
            
            # ÌååÏùºÎ™Ö Î≥ÄÍ≤Ω (ÎÇ†Ïßú Ï†úÍ±∞)
            if md_path.exists():
                new_filename = f"{data['title']}.md".replace(':', '').replace('/', '_').replace('\\', '_') # Sanitize title roughly
                new_path = md_path.parent / new_filename
                
                # Í∏∞Ï°¥ utils.sanitize_filename Î°úÏßÅÍ≥º Î∂àÏùºÏπòÌï† Ïàò ÏûàÏúºÎØÄÎ°ú
                # ÏÉùÏÑ±Îêú ÌååÏùºÏóêÏÑú ÎÇ†Ïßú Î∂ÄÎ∂ÑÎßå Ï†úÍ±∞ÌïòÎäî ÏãùÏúºÎ°ú Ï≤òÎ¶¨
                # Ïòà: [2026-01-11] Ï†úÎ™©.md -> Ï†úÎ™©.md
                
                clean_name = re.sub(r'^\[\d{4}-\d{2}-\d{2}\]\s*', '', md_path.name)
                new_path = md_path.parent / clean_name
                
                # Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎ©¥ ÎçÆÏñ¥Ïì∞Í±∞ÎÇò Î≤àÌò∏ Î∂ôÏù¥Í∏∞ (Ïó¨Í∏∞ÏÑ† ÎçÆÏñ¥Ïì∞Í∏∞ or Ìå®Ïä§)
                if new_path.exists():
                     new_path.unlink() # Ïò§Î≤ÑÎùºÏù¥Îìú
                
                md_path.rename(new_path)
                md_path = new_path

            saved_files.append(md_path)
            print(f"‚úÖ Transcript Saved: {md_path}")
            
        else:
            # Web: PDF + Optional Summary (Separate)
            
            # Generate PDF
            print("\n2Ô∏è‚É£  Generating PDF (with auto-cleanup)...")
            try:
                html_content = data.get('html_content')
                pdf_path = pdf_gen.save(data, html_content, source_html_path=None)
                
                # PDF ÌååÏùºÎ™Ö Î≥ÄÍ≤Ω (ÎÇ†Ïßú Ï†úÍ±∞)
                if pdf_path and pdf_path.exists():
                    clean_pdf_name = re.sub(r'^\[\d{4}-\d{2}-\d{2}\]\s*', '', pdf_path.name)
                    new_pdf_path = pdf_path.parent / clean_pdf_name
                    
                    if new_pdf_path.exists():
                        new_pdf_path.unlink() # Ïò§Î≤ÑÎùºÏù¥Îìú
                    
                    pdf_path.rename(new_pdf_path)
                    pdf_path = new_pdf_path
                    print(f"‚úÖ PDF Renamed: {pdf_path}")
                
                saved_files.append(pdf_path)
                print(f"‚úÖ PDF Saved: {pdf_path}")
            except Exception as e:
                print(f"‚ùå PDF Generation Failed: {e}")
                pdf_path = None # Ensure pdf_path is defined even if failure
            
            # Generate Summary (Separate File)
            if summarizer:
                print("\n3Ô∏è‚É£  Generating Summary (Gemini - Article Mode)...")
                if data.get('html_content'):
                    source_text = data['html_content']
                else:
                    source_text = data['content']
                
                # Prepare Metadata for Article
                
                # Clean URL (Remove Query Params)
                parsed_url = urlparse(data['url'])
                clean_url = parsed_url._replace(query=None).geturl()
                
                metadata = {
                    'created': data.get('publish_date') or datetime.now().strftime("%Y-%m-%d"),
                    'source': clean_url, 
                    # cleanÎêú PDF ÌååÏùºÎ™Ö Ï†ÑÎã¨
                    'pdf_filename': pdf_path.name if 'pdf_path' in locals() and pdf_path else "Unknown.pdf"
                }

                summary = summarizer.summarize_text(source_text, content_type='article', metadata=metadata)
                
                if summary:
                    summary_data = data.copy()
                    summary_data['title'] = f"{data['title']} (Summary)"
                    summary_data['content'] = summary
                    summary_data['type'] = f"{data['type']} - Summary"
                    
                    summary_path = md_gen.save(summary_data, image_processor=None)
                    
                     # ÌååÏùºÎ™Ö Î≥ÄÍ≤Ω (ÎÇ†Ïßú Ï†úÍ±∞) - Article Summary
                    if summary_path.exists():
                        clean_name = re.sub(r'^\[\d{4}-\d{2}-\d{2}\]\s*', '', summary_path.name)
                        new_path = summary_path.parent / clean_name
                        if new_path.exists():
                            new_path.unlink()
                        summary_path.rename(new_path)
                        summary_path = new_path

                    saved_files.append(summary_path)
                    print(f"‚úÖ Summary Saved: {summary_path}")
                else:
                    print("‚ùå Summary generation returned empty result.")

        # 4. Upload (Step 2)
        if uploader and drive_folder_id:
            print("\n4Ô∏è‚É£  Uploading to Google Drive...")
            print(f"   Target Folder ID: {drive_folder_id}")
            
            uploaded_count = 0
            for file_path in saved_files:
                file_id = uploader.upload_file(str(file_path), drive_folder_id)
                if file_id:
                    print(f"‚úÖ Uploaded {file_path.name} -> ID: {file_id}")
                    uploaded_count += 1
                else:
                    print(f"‚ùå Upload Failed: {file_path.name}")
            
            # File Cleanup (User Request)
            if uploaded_count > 0:
                print("\nüßπ Cleaning up test_output directory...")
                for file_path in saved_files:
                    try:
                        if file_path.exists():
                            file_path.unlink()
                            print(f"Deleted: {file_path.name}")
                    except Exception as e:
                        print(f"Failed to delete {file_path.name}: {e}")
                
        else:
            print("\n4Ô∏è‚É£  Skipping Upload (Missing Token or Folder ID)")

            
        print("\nüéâ Test Complete!")
        
    except Exception as e:
        print(f"\n‚ùå Test Failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_scraper()
